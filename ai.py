# ai.py

import httpx
import base64
from openai import AsyncOpenAI
from config import API_KEY, BASE_URL, MODEL, AI_GENERATE_BASE_PROMPT, LOVE_SYSTEM_PROMPT, AI_ANSWER_SYSTEM_PROMPT

# --- CLIENT INITIALIZATION ---
client = AsyncOpenAI(api_key=API_KEY, base_url=BASE_URL)
http_client = httpx.AsyncClient()

async def get_ai_vision_response(user_prompt: str, image_url: str, system_prompt: str) -> str:
    """
    Generates a response from the AI based on a text prompt and an image.
    It downloads the image, encodes it in Base64, and sends the data directly.
    Args:
        user_prompt: The text prompt related to the image.
        image_url: The public URL of the image from Telegram.
        system_prompt: The system message to guide the AI's behavior.
    Returns:
        The AI-generated text response as a string.
    """
    try:
        # Step 1: Asynchronously download the image content from the URL
        response = await http_client.get(image_url)
        response.raise_for_status()  # Raise an exception for bad status codes (like 404)
        image_bytes = response.content

        # Step 2: Encode the downloaded image content into a Base64 string
        base64_image = base64.b64encode(image_bytes).decode('utf-8')

        # Step 3: Call the AI API with the Base64 encoded image data
        api_response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_prompt},
                        {
                            "type": "image_url",
                            # This is the required format for sending Base64 image data
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            },
                        },
                    ],
                }
            ],
            stream=False,
            max_tokens=200
        )
        return api_response.choices[0].message.content
    except httpx.HTTPStatusError as http_err:
        print(f"HTTP error occurred while downloading image: {http_err}")
        return 'ä¸‹è¼‰å””åˆ°å¼µåœ–ï¼Œè«‹å†è©¦ä¸€æ¬¡'
    except Exception as e:
        print(f"Error in get_ai_vision_response: {e}")
        return 'ç³»çµ±åˆ†æå””åˆ°å¼µåœ–ï¼Œå¥½å°å””ä½'

async def get_ai_answer(user_prompt: str) -> str:
    """
    Generates a text-based answer from the AI using a specific system prompt for Q&A.
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": AI_ANSWER_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_answer: {e}")
        return 'ç³»çµ±æƒ³æ–¹åŠ (å‡ºéŒ¯)ï¼Œå¥½å°å””ä½'

async def get_ai_summary(user_prompt: str, system_prompt="") -> str:
    """
    Generates a text-based summary or response from the AI.
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": system_prompt if system_prompt else AI_GENERATE_BASE_PROMPT
                },
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_summary: {e}")
        return 'ç³»çµ±æƒ³æ–¹åŠ (å‡ºéŒ¯)ï¼Œå¥½å°å””ä½'

async def get_ai_apology() -> str:
    """
    Generates a humorous, Hong Kong-style apology.
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "user",
                 "content": "ç”¨ç¹é«”ä¸­æ–‡åŒé¦™æ¸¯å¼å£èªå»é“æ­‰ï¼Œè«‹äººé£Ÿäº”ä»æœˆé¤…ï¼Œæç¬‘ä½†å””æœƒå¾—ç½ªäººå˜…é“æ­‰ï¼Œè¦æœ‰å•²emojiï¼Œå­—æ•¸30ä»¥ä¸‹ï¼Œä¸ç”¨åŠ ä¸Šè¨»è§£"},
            ],
            stream=False
        )
        apology = response.choices[0].message.content
        apology += "\n\nå…è²¬è²æ˜: å””é—œäº”ä»æœˆé¤…äº‹ğŸ¥®æ±‚ä¸‹å¤§å®¶ä¿¾ä¸‹é¢ğŸ™"
        return apology
    except Exception as e:
        print(f"Error in get_ai_apology: {e}")
        return 'å“å‘€ï¼Œé“æ­‰å¤±æ•—ï¼Œå””å¥½æ‰“æˆ‘ğŸ™'

async def get_ai_love_quote(username: str, user_messages: str) -> str:
    """
    Generates a cheesy, Hong Kong-style love quote for a specific user.
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": LOVE_SYSTEM_PROMPT},
                {"role": "user", "content": f"username:{username}\tuser's messages:{user_messages}"},
            ],
            stream=False
        )
        love_quote = response.choices[0].message.content
        love_quote += "\n\nå…è²¬è²æ˜: åœŸå‘³æƒ…è©±ç´”å±¬å¨›æ¨‚ğŸ˜˜è«‹å‹¿ç•¶çœŸğŸ’–"
        return love_quote
    except Exception as e:
        print(f"Error in get_love_quote: {e}")
        return 'å“å‘€ï¼Œæƒ…è©±ç”Ÿæˆå¤±æ•—ï¼Œæ„›ä½ å””ä½¿è¬›ğŸ˜œ'

async def get_ai_countdown(user_prompt="") -> str:
    """
    Generates a creative countdown message.
    """
    try:
        response = await client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": AI_GENERATE_BASE_PROMPT},
                {"role": "user", "content": f"å†ä¿®é£¾ä»¥ä¸‹å¥å­ï¼Œä½¿å…§å®¹è®Šå¾—æœ‰è¶£: '{user_prompt}'"},
            ],
            stream=False
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_countdown: {e}")
        return f"éŒ¯æ’šæ›¬ï¼"
