# ai.py

import requests  # Use requests for synchronous HTTP calls
import base64
import json
from openai import OpenAI, APITimeoutError  # Use the synchronous OpenAI client
from config import (
    API_KEY,
    BASE_URL,
    MODEL,
    AI_GENERATE_BASE_PROMPT,
    LOVE_SYSTEM_PROMPT,
    AI_ANSWER_SYSTEM_PROMPT,
    SERPER_API_KEY,
    logger,
)

# --- CLIENT INITIALIZATION ---
client = OpenAI(api_key=API_KEY, base_url=BASE_URL)


# --- SERPER SEARCH FUNCTION ---
def search_with_serper(query: str, time_range: str = "qdr:y") -> str:
    """
    Search for information using Serper.dev API.

    Args:
        query: The search query
        time_range: Time range for results. Options:
            - "qdr:h" - past hour
            - "qdr:d" - past 24 hours (for real-time info like weather, news)
            - "qdr:w" - past week
            - "qdr:m" - past month
            - "qdr:y" - past year (default for general queries)

    Returns:
        Formatted search results as a string
    """
    try:
        logger.info(f"Searching with Serper: {query} (time_range: {time_range})")

        url = "https://google.serper.dev/search"
        payload = json.dumps(
            {
                "q": query,
                "num": 10,  # Get top 10 results
                "location": "Hong Kong",  # Get results from Hong Kong
                "gl": "hk",  # Get results from Hong Kong
                "hl": "zh-tw",  # Get results in Chinese
                "tbs": time_range,  # Time-based search
            }
        )
        headers = {"X-API-KEY": SERPER_API_KEY, "Content-Type": "application/json"}

        response = requests.post(url, headers=headers, data=payload, timeout=10)
        response.raise_for_status()

        data = response.json()
        logger.info("Serper search successful")

        # Format the search results
        search_results = []

        # Add organic results
        if "organic" in data:
            for idx, result in enumerate(data["organic"][:5], 1):
                title = result.get("title", "")
                snippet = result.get("snippet", "")
                link = result.get("link", "")
                search_results.append(f"{idx}. {title}\n   {snippet}\n   ä¾†æº: {link}")

        # Add knowledge graph if available
        if "knowledgeGraph" in data:
            kg = data["knowledgeGraph"]
            kg_title = kg.get("title", "")
            kg_desc = kg.get("description", "")
            if kg_title and kg_desc:
                search_results.insert(0, f"ğŸ“š çŸ¥è­˜åœ–è­œ: {kg_title}\n   {kg_desc}\n")

        if search_results:
            return "\n\n".join(search_results)
        else:
            return "æµå””åˆ°ç›¸é—œè³‡æ–™"

    except requests.exceptions.Timeout:
        logger.error("Serper API timeout")
        return "æœå°‹è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡"
    except requests.exceptions.RequestException as e:
        logger.error(f"Serper API error: {e}")
        return "æœå°‹å‡ºç¾å•é¡Œï¼Œè«‹å†è©¦ä¸€æ¬¡"
    except Exception as e:
        logger.error(f"Unexpected error in search_with_serper: {e}")
        return "æœå°‹ç³»çµ±å‡ºéŒ¯"


# --- VISION FUNCTION (SYNCHRONOUS) ---
def get_ai_vision_response(user_prompt: str, image_url: str, system_prompt: str) -> str:
    """
    Generates a response from the AI based on a text prompt and an image using synchronous calls.
    Includes enhanced logging to inspect the full API response.
    """
    logger.info("Starting get_ai_vision_response function.")
    try:
        logger.info(f"Downloading image from URL: {image_url}")
        response = requests.get(image_url, timeout=15)
        response.raise_for_status()
        image_bytes = response.content
        logger.info("Image downloaded successfully.")

        logger.info("Encoding image to Base64.")
        base64_image = base64.b64encode(image_bytes).decode("utf-8")
        logger.info("Image encoded successfully.")

        logger.info("Calling OpenAI API for vision response.")
        api_response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": user_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            },
                        },
                    ],
                },
            ],
            stream=False,
        )

        # --- NEW DEBUGGING LINES ---
        # Log the full, raw response object from the API to see its structure.
        logger.info(f"Full API Response: {api_response}")

        # Check if the response is valid before trying to access its content.
        if api_response.choices and api_response.choices[0].message:
            logger.info("OpenAI API call successful, content found.")
            return api_response.choices[0].message.content
        else:
            # This will now catch cases where the API returns 200 OK but an empty/filtered response.
            logger.error("API call was successful but returned no content/choices.")
            return "AI æˆåŠŸå›æ‡‰ï¼Œä½†å…§å®¹ä¿‚ç©ºå˜…ï¼Œå¯èƒ½ä¿‚å®‰å…¨è¨­å®šæ“‹å’—ã€‚"

    except requests.exceptions.Timeout:
        logger.error("Timeout occurred while downloading image.")
        return "ä¸‹è¼‰åœ–ç‰‡è¶…æ™‚ï¼Œå¼µç›¸å¯èƒ½å¤ªå¤§æˆ–è€…ç¶²çµ¡æœ‰å•é¡Œ"
    except requests.exceptions.RequestException as http_err:
        logger.error(f"HTTP error occurred while downloading image: {http_err}")
        return "ä¸‹è¼‰å””åˆ°å¼µåœ–ï¼Œè«‹å†è©¦ä¸€æ¬¡"
    except APITimeoutError:
        logger.error("OpenAI API call timed out.")
        return "AIè«—å¤ªè€è«—åˆ°ç“è‘—å’—ï¼Œè«‹å†è©¦ä¸€æ¬¡"
    except Exception as e:
        logger.error(f"An unexpected error occurred in get_ai_vision_response: {e}")
        # Log the actual API response object if an error occurs during parsing
        if "api_response" in locals():
            logger.error(
                f"API response at time of error: {locals().get('api_response')}"
            )
        return "ç³»çµ±åˆ†æå””åˆ°å¼µåœ–ï¼Œå¥½å°å””ä½"


# --- FUNCTION TOOLS DEFINITION ---
SEARCH_TOOL = {
    "type": "function",
    "function": {
        "name": "search_with_serper",
        "description": """æœå°‹ç¶²ä¸Šæœ€æ–°è³‡æ–™ã€‚ç•¶ç”¨æˆ¶å•é¡Œæ¶‰åŠä»¥ä¸‹æƒ…æ³æ™‚æ‡‰è©²ä½¿ç”¨æ­¤å·¥å…·ï¼š
- å¯¦æ™‚è³‡è¨Šï¼šå¤©æ°£ã€æ–°èã€è‚¡åƒ¹ã€åŒ¯ç‡ç­‰
- æœ€æ–°æ•¸æ“šï¼šç•¶å‰åƒ¹æ ¼ã€ä»Šæ—¥è³‡è¨Šã€æœ€è¿‘æ¶ˆæ¯
- äº‹å¯¦æŸ¥è­‰ï¼šéœ€è¦é©—è­‰å˜…è³‡æ–™ã€çµ±è¨ˆæ•¸å­—
- ä½ çŸ¥è­˜åº«ä»¥å¤–å˜…è³‡è¨Šï¼š2024å¹´å¾Œå˜…è³‡æ–™ã€æœ€æ–°ç™¼å±•

ä¸éœ€è¦æœå°‹å˜…æƒ…æ³ï¼š
- åŸºæœ¬æ¦‚å¿µè§£é‡‹ï¼ˆä¾‹å¦‚ï¼šé»è§£å¤©ç©ºä¿‚è—è‰²ï¼‰
- æ­·å²äº‹ä»¶ã€ç§‘å­¸åŸç†
- ç´”ç²¹æ„è¦‹ã€å»ºè­°é¡å•é¡Œ

âš ï¸ æ™‚é–“é‚è¼¯æª¢æŸ¥ï¼š
å¦‚æœæœå°‹çµæœä¸­å˜…æ—¥æœŸåŒç”¨æˆ¶æä¾›å˜…ç•¶å‰æ—¥æœŸæœ‰çŸ›ç›¾ï¼ˆä¾‹å¦‚ï¼šæœå°‹çµæœè©±ã€Œé è¨ˆ10æœˆ1æ—¥ã€ä½†è€Œå®¶å·²ç¶“10æœˆ10æ—¥ï¼‰ï¼Œä½ æ‡‰è©²ï¼š
1. å†æ¬¡æœå°‹æ›´æ–°è³‡è¨Šï¼ˆç”¨æ›´å…·é«”å˜…é—œéµè© + qdr:dï¼‰
2. æˆ–è€…å–ºå›ç­”ä¸­èªªæ˜å‘¢å€‹æ™‚é–“å·®ç•°""",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "æœå°‹æŸ¥è©¢å­—ä¸²ï¼Œæ‡‰è©²ç°¡æ½”ä¸”æº–ç¢ºã€‚ä¾‹å¦‚ï¼š'é¦™æ¸¯å¤©æ°£ 2025å¹´10æœˆ10æ—¥'ã€'æ¯”ç‰¹å¹£åƒ¹æ ¼'ã€'é¦™æ¸¯æœ€ä½å·¥è³‡ 2024'",
                },
                "time_range": {
                    "type": "string",
                    "enum": ["qdr:h", "qdr:d", "qdr:w", "qdr:m", "qdr:y"],
                    "description": """æœå°‹æ™‚é–“ç¯„åœï¼š
- 'qdr:h': éå»1å°æ™‚ï¼ˆæ¥µåº¦å¯¦æ™‚è³‡è¨Šï¼‰
- 'qdr:d': éå»24å°æ™‚ï¼ˆå¤©æ°£ã€ä»Šæ—¥æ–°èã€ç•¶æ—¥è‚¡åƒ¹ï¼‰
- 'qdr:w': éå»1æ˜ŸæœŸï¼ˆæœ€è¿‘æ–°èã€çŸ­æœŸè¶¨å‹¢ï¼‰
- 'qdr:m': éå»1å€‹æœˆï¼ˆè¿‘æœŸç™¼å±•ï¼‰
- 'qdr:y': éå»1å¹´ï¼ˆä¸€èˆ¬æŸ¥è©¢ï¼Œé è¨­å€¼ï¼‰

é¸æ“‡å»ºè­°ï¼š
- å¤©æ°£ã€ä»Šæ—¥æ–°è â†’ qdr:d
- å³æ™‚è‚¡åƒ¹ã€åŒ¯ç‡ â†’ qdr:h æˆ– qdr:d
- æœ€è¿‘æ”¿ç­–ã€è¶¨å‹¢ â†’ qdr:m
- ä¸€èˆ¬çŸ¥è­˜ã€éæ™‚æ•ˆæ€§è³‡è¨Š â†’ qdr:y""",
                    "default": "qdr:y",
                },
            },
            "required": ["query"],
        },
    },
}


# --- TEXT-ONLY FUNCTIONS (SYNCHRONOUS) ---
def get_ai_answer_with_tools(user_prompt: str, max_iterations: int = 3) -> str:
    """
    Generates a text-based answer from the AI with tool calling capability.
    AI can decide whether to search for information using Serper.
    Supports iterative tool calling - AI can search, analyze results, and search again if needed.

    Args:
        user_prompt: The user's question
        max_iterations: Maximum number of tool calling iterations (default: 3)
    """
    try:
        logger.info(f"Starting get_ai_answer_with_tools for prompt: {user_prompt}")

        messages = [
            {"role": "system", "content": AI_ANSWER_SYSTEM_PROMPT},
            {"role": "user", "content": user_prompt},
        ]

        iteration = 0
        while iteration < max_iterations:
            iteration += 1
            logger.info(f"Tool calling iteration {iteration}/{max_iterations}")

            # Call AI - may decide to use tools
            response = client.chat.completions.create(
                model=MODEL,
                messages=messages,
                tools=[SEARCH_TOOL],
                tool_choice="auto",  # Let AI decide
                stream=False,
            )

            response_message = response.choices[0].message
            tool_calls = response_message.tool_calls

            # If AI wants to use tools
            if tool_calls:
                logger.info(
                    f"AI requested {len(tool_calls)} tool call(s) in iteration {iteration}"
                )

                # Add AI's response to messages
                messages.append(response_message)

                # Execute each tool call
                for tool_call in tool_calls:
                    function_name = tool_call.function.name
                    function_args = json.loads(tool_call.function.arguments)

                    logger.info(f"Calling {function_name} with args: {function_args}")

                    if function_name == "search_with_serper":
                        query = function_args.get("query")
                        time_range = function_args.get("time_range", "qdr:y")

                        # Call the search function
                        search_results = search_with_serper(query, time_range)

                        # Add function result to messages
                        messages.append(
                            {
                                "tool_call_id": tool_call.id,
                                "role": "tool",
                                "name": function_name,
                                "content": search_results,
                            }
                        )

                # Continue loop - AI will process results and may call tools again
                continue

            else:
                # AI decided it has enough information, return final answer
                logger.info(f"AI provided final answer after {iteration} iteration(s)")
                return response_message.content

        # Max iterations reached, return whatever AI has
        logger.warning(f"Max iterations ({max_iterations}) reached")
        return (
            response_message.content
            if response_message.content
            else "ç³»çµ±è™•ç†è¶…æ™‚ï¼Œè«‹ç°¡åŒ–å•é¡Œå†è©¦"
        )

    except Exception as e:
        logger.error(f"Error in get_ai_answer_with_tools: {e}")
        return "ç³»çµ±æƒ³æ–¹åŠ (å‡ºéŒ¯)ï¼Œå¥½å°å””ä½"


def get_ai_answer(user_prompt: str, search_results: str = None) -> str:
    """
    Generates a text-based answer from the AI.
    If search_results are provided, they will be included in the prompt.

    Note: This is the legacy function. Use get_ai_answer_with_tools() for automatic tool calling.
    """
    try:
        # If search results are provided, include them in the prompt
        if search_results:
            enhanced_prompt = f"""ç”¨æˆ¶å•é¡Œï¼š{user_prompt}

ä»¥ä¸‹ä¿‚ç¶²ä¸Šæœå°‹åˆ°å˜…æœ€æ–°ç›¸é—œè³‡æ–™ï¼ˆåƒ…ä¾›åƒè€ƒï¼‰ï¼š
{search_results}

---

è«‹çµåˆä½ å˜…çŸ¥è­˜åº«åŒä¸Šè¿°æœå°‹è³‡æ–™ï¼Œç¶“éåˆ†æåŒæ€è€ƒå¾Œï¼Œç”¨å»£æ±è©±å›ç­”ç”¨æˆ¶å˜…å•é¡Œã€‚

é‡è¦æç¤ºï¼š
- æœå°‹è³‡æ–™åªä¿‚åƒè€ƒææ–™ï¼Œå””ä¿‚è¦ä½ ç…§æŠ„
- é‹ç”¨ä½ å˜…çŸ¥è­˜å»ç†è§£ã€åˆ†æã€è£œå……æœå°‹çµæœ
- æä¾›æœ‰è¦‹åœ°å˜…ç­”æ¡ˆï¼Œå””ä¿‚å–®ç´”è½‰è¿°è³‡æ–™
- å¦‚ç”¨å’—æœå°‹è³‡æ–™ï¼Œå¯ç°¡å–®è¨»æ˜ã€Œæ ¹æ“šæœ€æ–°è³‡æ–™ã€"""
        else:
            enhanced_prompt = user_prompt

        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": AI_ANSWER_SYSTEM_PROMPT},
                {"role": "user", "content": enhanced_prompt},
            ],
            stream=False,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_answer: {e}")
        return "ç³»çµ±æƒ³æ–¹åŠ (å‡ºéŒ¯)ï¼Œå¥½å°å””ä½"


def get_ai_summary(user_prompt: str, system_prompt="") -> str:
    """
    Generates a text-based summary or response from the AI.
    """
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "system",
                    "content": (
                        system_prompt if system_prompt else AI_GENERATE_BASE_PROMPT
                    ),
                },
                {"role": "user", "content": user_prompt},
            ],
            stream=False,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_summary: {e}")
        return "ç³»çµ±æƒ³æ–¹åŠ (å‡ºéŒ¯)ï¼Œå¥½å°å””ä½"


def get_ai_apology() -> str:
    """
    Generates a humorous apology.
    """
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {
                    "role": "user",
                    "content": "ç”¨ç¹é«”ä¸­æ–‡åŒé¦™æ¸¯å¼å£èªå»é“æ­‰ï¼Œè«‹äººé£Ÿäº”ä»æœˆé¤…ï¼Œæç¬‘ä½†å””æœƒå¾—ç½ªäººå˜…é“æ­‰ï¼Œè¦æœ‰å•²emojiï¼Œå­—æ•¸30ä»¥ä¸‹ï¼Œä¸ç”¨åŠ ä¸Šè¨»è§£",
                },
            ],
            stream=False,
        )
        apology = response.choices[0].message.content
        apology += "\n\nå…è²¬è²æ˜: å””é—œäº”ä»æœˆé¤…äº‹ğŸ¥®æ±‚ä¸‹å¤§å®¶ä¿¾ä¸‹é¢ğŸ™"
        return apology
    except Exception as e:
        print(f"Error in get_ai_apology: {e}")
        return "å“å‘€ï¼Œé“æ­‰å¤±æ•—ï¼Œå””å¥½æ‰“æˆ‘ğŸ™"


def get_ai_love_quote(username: str, user_messages: str) -> str:
    """
    Generates a cheesy love quote.
    """
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": LOVE_SYSTEM_PROMPT},
                {
                    "role": "user",
                    "content": f"username:{username}\tuser's messages:{user_messages}",
                },
            ],
            stream=False,
        )
        love_quote = response.choices[0].message.content
        love_quote += "\n\nå…è²¬è²æ˜: åœŸå‘³æƒ…è©±ç´”å±¬å¨›æ¨‚ğŸ˜˜è«‹å‹¿ç•¶çœŸğŸ’–"
        return love_quote
    except Exception as e:
        print(f"Error in get_love_quote: {e}")
        return "å“å‘€ï¼Œæƒ…è©±ç”Ÿæˆå¤±æ•—ï¼Œæ„›ä½ å””ä½¿è¬›ğŸ˜œ"


def get_ai_countdown(user_prompt="") -> str:
    """
    Generates a creative countdown message.
    """
    try:
        response = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": AI_GENERATE_BASE_PROMPT},
                {
                    "role": "user",
                    "content": f"å†ä¿®é£¾ä»¥ä¸‹å¥å­ï¼Œä½¿å…§å®¹è®Šå¾—æœ‰è¶£: '{user_prompt}'",
                },
            ],
            stream=False,
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error in get_ai_countdown: {e}")
        return f"éŒ¯æ’šæ›¬ï¼"
